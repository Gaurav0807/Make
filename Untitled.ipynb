{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372eeeeb",
   "metadata": {},
   "source": [
    "# Finreference ,Due_Date,ReceiptDate,Status=R\n",
    "# Due_Date >= ReceiptDate onCycle '1'\n",
    "# Due_Date < ReceiptDate onCycle='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c6b32dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Mahindra-main') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39d9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cdjobs_receipt_hourly_snapshot.csv',header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294581d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.select('LOAN_ACCOUNT_NUMBER','RECEIPT_STATUS','HOURLY_TIME_STAMP')\n",
    "df1 = df1.withColumnRenamed('HOURLY_TIME_STAMP','Receipt_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718c9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/New_cd_jobs_loan_monthly_snapshot.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f6b6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.select('Loan_Account_Number','Due_Date','Bucket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5dbf6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df1.join(df2,df1.LOAN_ACCOUNT_NUMBER==df2.Loan_Account_Number,'right_outer').drop(df2.Loan_Account_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "45cce11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.csv('C:/Users/Gaurav/Desktop/Data_Check/AdditionalOutput', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f32ef06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/AdditionalOutput/a11.csv',header=True,inferSchema=True)\n",
    "#df3 = df3.withColumn('Receipt_Date1',lit('Null'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c905df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------+-------------+------+\n",
      "|LOAN_ACCOUNT_NUMBER|RECEIPT_STATUS|  Due_Date|Receipt_Date1|Bucket|\n",
      "+-------------------+--------------+----------+-------------+------+\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|         null|     0|\n",
      "+-------------------+--------------+----------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df3.select('LOAN_ACCOUNT_NUMBER','RECEIPT_STATUS','Due_Date','Receipt_Date1','Bucket')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "573be77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.fillna({'Receipt_Date1':'05-02-2022'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56b6cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------+-------------+------+-------+\n",
      "|LOAN_ACCOUNT_NUMBER|RECEIPT_STATUS|  Due_Date|Receipt_Date1|Bucket|OnCycle|\n",
      "+-------------------+--------------+----------+-------------+------+-------+\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|\n",
      "+-------------------+--------------+----------+-------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df3 = df3.withColumn('OnCycle', when((col('RECEIPT_STATUS')=='R') \n",
    "                                     & (col('Receipt_Date1')<=col('Due_Date')),1).otherwise(0))\n",
    "\n",
    "#df3.filter(col('OnCycle')==0).show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf5a36",
   "metadata": {},
   "source": [
    "# Normax => if cycle!=1  and  and Receipt_Date1 <=MonthLastDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7de4b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------+-------------+------+-------+------------+\n",
      "|LOAN_ACCOUNT_NUMBER|RECEIPT_STATUS|  Due_Date|Receipt_Date1|Bucket|OnCycle|monthEnddate|\n",
      "+-------------------+--------------+----------+-------------+------+-------+------------+\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|\n",
      "+-------------------+--------------+----------+-------------+------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df3.withColumn('monthEnddate', last_day((from_unixtime(unix_timestamp(lit(datetime.datetime.now().strftime(\"%Y-%m-%d\")), 'yyyy-MM-dd')).cast('date'))))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c568a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.coalesce(1).write.csv('C:/Users/Gaurav/Desktop/Data_Check/AdditionalOutput1', header=True)\n",
    "#df3= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/AdditionalOutput1/check.csv',header=True,inferSchema=True)\n",
    "df3 = df3.withColumn('Normax',when((col('OnCycle')!=1) & (col('RECEIPT_STATUS')!='R')\n",
    "                                      & (col('Receipt_Date1')<=col('monthEnddate')),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa924435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------+-------------+------+-------+------------+------+\n",
      "|LOAN_ACCOUNT_NUMBER|RECEIPT_STATUS|  Due_Date|Receipt_Date1|Bucket|OnCycle|monthEnddate|Normax|\n",
      "+-------------------+--------------+----------+-------------+------+-------+------------+------+\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "|     CRO0CD00001673|             R|10-02-2022|   05-02-2022|     0|      1|  2022-02-28|     0|\n",
      "+-------------------+--------------+----------+-------------+------+-------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()\n",
    "df3.coalesce(1).write.csv('C:/Users/Gaurav/Desktop/Data_Check/Output', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee2f57",
   "metadata": {},
   "source": [
    "# Balance_Builder\n",
    "# last month bucket greater than Zero and current month Bucket is Zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "be5620b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------+\n",
      "|   LOAN_ID|  DUE_DATE| PAID_DATE|BUCKET|\n",
      "+----------+----------+----------+------+\n",
      "|1000012712|2021-08-10|      null|     4|\n",
      "|1000030737|2021-08-05|2021-09-02|     3|\n",
      "|1000102666|2021-08-05|      null|     4|\n",
      "|1000143742|2021-08-05|      null|     3|\n",
      "|1000148615|2021-08-01|      null|     4|\n",
      "|1000160080|2021-08-10|      null|     4|\n",
      "|1000169816|2021-08-05|      null|     4|\n",
      "|1000176402|2021-08-10|      null|     4|\n",
      "|1000191209|2021-08-10|      null|     2|\n",
      "|1000235810|2021-08-10|      null|     4|\n",
      "|1000236992|2021-08-10|      null|     4|\n",
      "|1000248105|2021-08-10|2021-08-16|     0|\n",
      "|1000270979|2021-08-10|      null|     4|\n",
      "|1000294038|2021-08-10|      null|     3|\n",
      "|1000320258|2021-08-05|2021-08-06|     0|\n",
      "|1000346176|2021-08-10|2021-08-11|     0|\n",
      "|1000347465|2021-08-10|      null|     4|\n",
      "|1000355470|2021-08-05|      null|     2|\n",
      "|1000372140|2021-08-10|      null|     4|\n",
      "|1000383563|2021-08-10|      null|     4|\n",
      "+----------+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.csv('C:/Users/Gaurav/Desktop/Data/FairCent.csv',header=True,inferSchema=True)\n",
    "df4 = df4.select('LOAN_ID','DPD','DUE_DATE','PAID_DATE')\n",
    "df4 = df4.withColumn('BUCKET', when(col('DPD') == 0, 0).when((col('DPD') > 0) & (col('DPD') <= 30), 1).when(\n",
    "        (col('DPD') >= 31) & (col('DPD') <= 60), 2).when((col('DPD') >= 61) & (col('DPD') <= 89), 3).when(col('DPD') >= 90, 4))\n",
    "\n",
    "df4 = df4.select('LOAN_ID', 'DUE_DATE', 'PAID_DATE', 'BUCKET')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "63554cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Mahindra-main') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d91ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cd_jobs_geographical_details.csv',header=True,inferSchema=True)\n",
    "df2= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cdjobs_receipt_hourly_snapshot.csv',header=True,inferSchema=True)\n",
    "\n",
    "\n",
    "df3= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cdjobs_loan_info_disbursement.csv',header=True,inferSchema=True)\n",
    "\n",
    "df4= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cdjobs_loan_monthly_snapshot.csv',header=True,inferSchema=True)\n",
    "df5= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cdjobs_receipt_hourly_snapshot.csv',header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d2350b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4 = df4.withColumn(\"Due_date1\",to_date(col(\"Due_Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b4ada358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loan_Account_Number',\n",
       " 'Customer_ID',\n",
       " 'Disbursement_Date',\n",
       " 'Disbursement_Amount',\n",
       " 'Tenure',\n",
       " 'Interest Rate',\n",
       " 'Installment',\n",
       " 'First Due Date',\n",
       " 'Last Due Date',\n",
       " 'Loan_Type',\n",
       " 'Product',\n",
       " 'Segment',\n",
       " 'Acquistion Channel',\n",
       " 'Security if available',\n",
       " 'Co-Applicant if applicable']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3436cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/New_cd_jobs_loan_monthly_snapshot.csv',header=True,inferSchema=True).select(\"DPD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c848c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"BUCKET_MOVEMENT\", when((col(\"DPD\")==0),\"NL\").otherwise(\"RF\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3491b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|DPD|BUCKET_MOVEMENT|\n",
      "+---+---------------+\n",
      "|  0|             NL|\n",
      "|  8|             RF|\n",
      "|  8|             RF|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  8|             RF|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "| 69|             RF|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  0|             NL|\n",
      "|  8|             RF|\n",
      "|  0|             NL|\n",
      "|  8|             RF|\n",
      "+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57878585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/cdjobs_receipt_hourly_snapshot.csv',header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ceebe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.select(\"LOAN_ACCOUNT_NUMBER\",\"HOURLY_TIME_STAMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1972606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df5 = df5.withColumn(\"Receipt_Date\",date_format(col(\"HOURLY_TIME_STAMP\"),\"yyyy-MM-dd\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee3cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.withColumn(\"Receipt\",from_unixtime(unix_timestamp(col(\"HOURLY_TIME_STAMP\"),\"yyyy-MM-dd'T'HH:mm:ss.SSS\"),\"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7f5b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5= df5.withColumn(\"Due_date1\",to_date(col(\"HOURLY_TIME_STAMP\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ed521ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+-------+---------+\n",
      "|LOAN_ACCOUNT_NUMBER|  HOURLY_TIME_STAMP|Receipt_Date|Receipt|Due_date1|\n",
      "+-------------------+-------------------+------------+-------+---------+\n",
      "|     HO00CD00000011|02/15/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000074|03/15/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000077|03/17/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000096|03/17/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000073|03/17/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000072|03/19/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000072|03/19/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000072|03/19/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000094|03/15/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000082|03/15/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000071|03/15/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000015|04/22/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000250|04/22/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000216|04/22/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000143|04/23/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000066|04/24/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000181|04/25/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000003|05/02/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000043|05/02/2020:00:00:00|        null|   null|     null|\n",
      "|     HO00CD00000045|05/02/2020:00:00:00|        null|   null|     null|\n",
      "+-------------------+-------------------+------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "822f7de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_ID: integer (nullable = true)\n",
      " |-- Bounce_Date: string (nullable = true)\n",
      " |-- DPD: integer (nullable = true)\n",
      " |-- Bucket: integer (nullable = true)\n",
      " |-- DUE_DATE: date (nullable = true)\n",
      " |-- beginning_of_month_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "d1= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/Dummy.csv',header=True,inferSchema=True)\n",
    "#a1 = datetime.today().date()\n",
    "#a2 = a1.replace(day=1)\n",
    "#d1 = d1.withColumn(\"Due_Date\",col(\"Due_Date\").cast(\"date\"))\n",
    "d1 = d1.withColumn(\"DUE_DATE\",to_date(col(\"DUE_DATE\"),'dd-MM-yyyy'))\n",
    "d1 = d1.withColumn(\"beginning_of_month_date\", trunc(col(\"Due_Date\"), \"month\"))\n",
    "d1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e443b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=d1.withColumn(\"Balance_Builder\",when((col(\"Bucket\")>0),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d7d0cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+------+----------+-----------------------+---------------+\n",
      "|Loan_ID|Bounce_Date|DPD|Bucket|  DUE_DATE|beginning_of_month_date|Balance_Builder|\n",
      "+-------+-----------+---+------+----------+-----------------------+---------------+\n",
      "|   1234| 01-02-2022| 90|     3|2020-01-10|             2020-01-01|              1|\n",
      "|   1234| 01-02-2022|  0|     0|2020-02-10|             2020-02-01|              0|\n",
      "|   1234| 01-02-2022| 30|     1|2020-03-10|             2020-03-01|              1|\n",
      "|   1234| 01-02-2022| 90|     3|2020-04-10|             2020-04-01|              1|\n",
      "|   1234| 01-02-2022|120|     4|2020-05-10|             2020-05-01|              1|\n",
      "|   1234| 01-02-2022|120|     4|2020-06-10|             2020-06-01|              1|\n",
      "|   1234| 01-02-2022|120|     4|2020-07-10|             2020-07-01|              1|\n",
      "|   1234| 01-02-2022| 60|     2|2020-08-10|             2020-08-01|              1|\n",
      "|   1234| 01-02-2022|  0|     0|2020-09-10|             2020-09-01|              0|\n",
      "|   1234| 01-02-2022|  0|     0|2020-10-10|             2020-10-01|              0|\n",
      "|   1234| 01-02-2022|  0|     0|2020-11-10|             2020-11-01|              0|\n",
      "|   1234| 01-02-2022| 60|     2|2020-12-10|             2020-12-01|              1|\n",
      "|   1234| 01-02-2022| 60|     2|2021-01-10|             2021-01-01|              1|\n",
      "|   1234| 01-02-2022| 60|     2|2021-02-10|             2021-02-01|              1|\n",
      "|   1234| 01-02-2022| 30|     1|2021-03-10|             2021-03-01|              1|\n",
      "|   1234| 01-02-2022| 60|     2|2021-04-10|             2021-04-01|              1|\n",
      "|   1234| 01-02-2022| 90|     3|2021-05-10|             2021-05-01|              1|\n",
      "|   1234| 01-02-2022|  0|     0|2021-06-10|             2021-06-01|              0|\n",
      "|   1234| 01-02-2022|  0|     0|2021-07-10|             2021-07-01|              0|\n",
      "|   1234| 01-02-2022| 30|     1|2021-08-10|             2021-08-01|              1|\n",
      "+-------+-----------+---+------+----------+-----------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9709233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f5= spark.read.csv('C:/Users/Gaurav/Desktop/Data_Check/testing.csv',header=True).select(\"RECEIPTDATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bc85ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#f5 = f5.withColumn(\"DUE_DATE\", to_date(col(\"DUE_DATE\"), 'dd-MM-yyyy'))\n",
    "f5 = f5.withColumn(\"RECEIPTDATE\",split(col(\"RECEIPTDATE\"),\":\").getItem(0))\n",
    "f5 = f5.withColumn(\"RECEIPTDATE1\", to_date(col(\"RECEIPTDATE\"), 'dd/MM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d83c991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|RECEIPTDATE|RECEIPTDATE1|\n",
      "+-----------+------------+\n",
      "| 11/12/2021|  2021-12-11|\n",
      "| 10/01/2021|  2021-01-10|\n",
      "| 01/01/2022|  2022-01-01|\n",
      "| 01/01/2022|  2022-01-01|\n",
      "| 12/05/2021|  2021-05-12|\n",
      "| 11/01/2021|  2021-01-11|\n",
      "| 01/01/2022|  2022-01-01|\n",
      "| 11/10/2021|  2021-10-11|\n",
      "| 12/02/2021|  2021-02-12|\n",
      "| 01/01/2022|  2022-01-01|\n",
      "| 11/03/2021|  2021-03-11|\n",
      "| 12/02/2021|  2021-02-12|\n",
      "| 11/19/2021|        null|\n",
      "| 12/02/2021|  2021-02-12|\n",
      "| 12/01/2021|  2021-01-12|\n",
      "| 10/01/2021|  2021-01-10|\n",
      "| 10/31/2021|        null|\n",
      "| 12/02/2021|  2021-02-12|\n",
      "| 11/03/2021|  2021-03-11|\n",
      "| 12/02/2021|  2021-02-12|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bb662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
